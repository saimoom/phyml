# Extend PhyML to use the BEAGLE library #

Much of the speed in PhyML comes from heuristic search techniques to efficiently search the space of possible trees. Parallelization is limited to MPI-based distribution of bootstrap replicates. Like most such software it uses the Felsenstein Pruning algorithm to compute the likelihood of individual trees using continuous-time Markov models. Recently considerable advances has been made in the fine-scale parallelization of this algorithm, in particular targeting massively parallel hardware such as NVidia GPGPUs (general purpose graphics processing units). There are considerable improvements in overall speed to be gained for PhyML by combining efficient search strategies with high speed likelihood computation.

BEAGLE is a cross-platform library that implements Felsenstein algorithm on a range of parallel and vector hardware including CUDA-based GPGPUs and SSE instructions on Intel chips:

http://code.google.com/p/beagle-lib/

This project involves extending PhyML to make calls to the BEAGLE library in place of the internal likelihood calculations. The project will involve: 1) Becoming familiar with the API of BEAGLE and linking of client software to the library (an example simple client in C is available in the BEAGLE package). 2) Understanding the likelihood calculations in PhyML. 3) Replacing likelihood calculation calls in PhyML with homologous calls to BEAGLE. 4) Testing and validation. 5) Performance testing with different hardware.

_This project was first proposed by Andrew Rambaut and listed by NESCENT among the proposals offered to students for GSoC2013. Imran Fanaswala's application was successful. Andrew Rambaut, Marc Suchard and Stephane Guindon will supervise Imran's work._

## Update1 ##
  * In PhyML, Replace Update\_P\_Lk() with homologous BEAGLE call(s). Essentially this means PhyML will use BEAGLE for its partial likelihood calculation.
  * Figured out why the spr.c is throwing an error
  * How large (if any) is the performance gain from SetTransitionMatrices() versus SetTransitionMatrix(). The latter is the "first step", but if BEAGLE authors claim the former is faster then maybe its worth juggling the code in order to use it?

## Update2 ##
  * Identified and worked on several Issues ( PhyML: http://bit.ly/13ms2JN, BEAGLE: http://bit.ly/12A4wHg )
  * Initial values of Brent's algorithm were inconsistent across binaries. This resulted in an extra iteration between runs, which then triggered a premature cutoff. The "culprit" was ultimately the gcc flags `fomit-frame-pointer` and `funroll-loops`
  * Created an overload for aLRT() that (re)uses the existing in memory tree
  * Created project specific Valgrind suppression files and `.gdbinit` files. The latter facilitates invoking functions without touching the binary (since some of the issues were not reproducible). Additionally, PhyML has hierarchy of structs with many pointer fields, which thus requires "deep printing". This was done using gdb's Python scripting
  * Likelihood calculations work now for a user provided tree (w/ branch lengths). This didn't work earlier due to a incorrect tip-numbering (i.imgur.com/Rgwa6yh.png). BTW, PhyML-BEAGLE now uses `SetTipPartials()` instead of `SetTipStates()`
  * Work in progress on optimizing branch lengths, topology, and finally estimation branch supports.
  * Merged, and tested, with new changes from trunk.

## Update3 ##
  * (Re)Implemented a client side scheme for indexing the appropriate GPU buffer. Details: PhyML uses both post and pre order traversals, which essentially means we need twice the number of partial vectors. Moreover, we needed a client side scheme for indexing the appropriate vector
  * Branch supports and branch lengths are estimated
  * Worked on SPR and NNI. While pruning and grafting subtrees, the likelihood vectors are shifted around; these actions need corresponding BEAGLE operations. The easy/lazy solution would be to simply call `beagleSetPartials()`, but this would allocate new memory rather than juggle pointers. The aforementioned indexing scheme, after a couple of revision, now cleanly allows this.


## Update4 ##
  * Previously, while optimizing topology we created two BEAGLE instances (with different rate categories) and discarded the first one after recording the topology. But now, we only use one instance with all but set the weights of the "unnecessary" categories to 0.0
  * As of now, all partial likelihoods are computed with BEAGLE for all combinations of parameters (i.e. branch length, topology, ts/tv ratio, rates, etc)
  * Code cleanup and commenting

## Update5 ##
  * Enabled rescaling of partials (not so efficient yet; work-in-progress)
  * Merged with SVN trunk
  * Re-read some literature

## Update6 ##
  * Tested and achieved bit-wise compatability on AA datasets
  * Update batch test script
  * Discussion about scaling implementation. Relegated for the time being

## Update 7 ##
  * On a fixed tree, P-matrix is computed by BEAGLE using UpdateTransitionMatricies()/SetEigenDecomposition()
  * run\_datasets.py now extracts and appends execution timings to the same logfile

## Update 8 ##
  * While optimizing branch lengths, P-matrix is computed by BEAGLE
  * Worked on incorporating PhyML's scaling technique into BEAGLE
  * Worked on create a homolog of Lk\_Core() which computes Edge likelihoods using BEAGLE

## Update 9 ##
  * Edgelikelihoods and P-matrices are computed using Beagle for NT and AA datasets (while optimizing all parameters)
  * Scaling happens at every node
  * Ran benchmarks

## Next Steps ##
  * Incorporate invariant sites (using the rate categories "trick")
  * why does BEAGLE need the FLOATING\_POINT\_ERROR check? For the time being that bit of code has been commented out
  * Currently transition matrices are being updated individually to resemble the original PhyML code. In other words, `UpdateTransitionMatrices()` updates a single matrix. Consider surgically extracting this to the "outer stack" so that all matrices are updated in one go (this also prevents many unnecessary calls to update the rate-across-sites and eigen-decompositions)
  * Do we need a "local" copy of the transition matrices? That is, do we need to call `GetTransitionMatrix()` since we are computing edgelikelihoods with BEAGLE?
  * The JC69 and K80 models doesn't allow unequal state frequencies, yet PhyML allows to explicitly specific frequencies using -f. This is conceptually "wrong". Nonetheless, when using these models with unequal state freqs the final lk betwene PhyML and PhyML+BEAGLE doesn't match. Maybe raise an error and exit rather than dig into this discrepency?
  * Shouldn't need to call `update_ras()` and `update_efrq()` in create\_beagle\_instance()
  * Check if `cur_site_lk` is used anywhere other than for computing the Edge lk, if not, then there is no need to call `getSiteLogLikelihood()`
  * Free(b\_fcs->Pij\_rr) before calling GetTransitionMatrix()?
  * Investigate `PMat_Zero_Br_Len()`... mostly likely not needed
  * Handle `Warn_and_Exit()` cases
  * Update `Print_Edge_Pmats()` and `Print_Edge_Likelihoods()`
  * Can `P_LK_LIM_INF` be represented when phydbl is a float?

### OLD NOTES ###
  1. First of all, keep in mind that the scaling is manually done per Stephane's suggestion (as it is quite subtle from what I've been told). Though in my latest push (PhyML's beagle branch), I have **disabled** the scaling code completely for PhyML (i.e. in `Update_P_Lk()` ) and in the BEAGLE interface (i.e. in `update_partials_beagle()`). Why, you ask?
    * In the BEAGLE interface (i.e. `update_partials_beagle()` ), the `curr_scaler` hits 4294967296 (2^32), but this does NOT happen in the PhyML (i.e. `Update_P_Lk()`)... eventhogh the actual code is the same. ( Of course, I only found this out on a non-trivial dataset while diffing a large memory dump... but anyway )
    * For the issue I will explain below, disabling the scaling seems to have no effect... so I try to reduce the confounding factors and disable it. So lets proceed, shall we? ok...
  1. For the toy dataset, and a **fixed** tree (i.e. no ratio tests, no parameter optimizations), I get the exact same final likelihoods in PhyML and BEAGLE. Good! However, for a larger dataset, I get slightly different likelihoods. Specifically, the following two commands yield different final likelihoods:
> > `./src/phyml-beagle -i ./datasets/17.codon.paml -d nt -q -c 4 -v 0 -t e -m JC69 -f '0.25,0.25,0.25,0.25' -o none -b 0 --r_seed 1999`
> > Final Lk: -17709.873912
> > `./src/phyml             -i ./datasets/17.codon.paml -d nt -q -c 4 -v 0 -t e -m JC69 -f '0.25,0.25,0.25,0.25' -o none -b 0 --r_seed 1999`
> > Final Lk: -17690.439032
    * I have artificially replaced gaps in the toy dataset and 17.codon.paml datasets (i.e. there are no ambigious characters)
    * Both datasets have a "crunched" sequence
    * Recall that the scaling code has been disabled, so it cant be that, right?
    * Recall that BEAGLE is supplied the P-Matrices (via SetTransitionMatrix()) for each branch. Thus the issue of rates doesn't even arise.
    * So thus, the BEAGLE callchain is SetTransitionMatrix() --> UpdatePartials() --> GetPartials() .. thats it. So... why are the final likelihoods different for the 17.codon.paml dataset? any ideas?
  1. Of course, I investigated the above question myself... I compared the P-matrices and the Partials on each edge. What did I find? Remember earlier I told you that the toy dataset gave the exact same final likelihoods? This is true. But, when I print the partials on each edge ... I notice that the partials on "left subtree of Branch 2" are different in PhyML and BEAGLE. In PhyML, the partials at the "left subtree" of Branch2 and Branch3 are the same... but in BEAGLE they are different. Yet... I still get the same final likelihood!
  1. Is it possible that I am simply just using BEAGLE incorrectly? [Here](http://codepad.org/ZyOniPkP) you can see the PhyML partial likelihood function and its BEAGLE homolog. Correct me if I am wrong, PhyML stores partials and P-matrices on the Edges rather than on the Nodes. In other words, each Edge struct has a `edge->p_lk_left/right` vector representing the partials on the left and right subtree respectively and a `edge->Pij` matrix (of dimension rate\*state\*state). PhyML then calls Update\_P\_Lk(d,b) which "updates partial likelihood on edge b on the side of b where node d lies". Ok so far so good? Next, I create a homolog BEAGLE function update\_partial\_pk(d,b) which does the same thing with the operation: `BeagleOperation operations[1] = {{d->num, BEAGLE_OP_NONE, BEAGLE_OP_NONE, n_v1->num, b1->num, n_v2->num, b2->num}};` Observe that the **child partials are indexed by the nodes(i.e. n\_v1, n\_v2) but the child transition matrixes are indexed by the edges (i.e. b1, b2)**. Does this make sense?
  1. A follow up... partials computed in `calcPartialsPartials()` don't match PhyML. Is this a hint?

  * Currently, I am providing the PMat to BEAGLE (via beagleSetTransitionMatrix()) in the Update\_PMat\_At\_Given\_Edge() because thats where the PMat is created. However, there are several other places where the PMat() is also called; for example, in M4\_Integral\_Term\_On\_One\_Edge() and even Print\_Model(). What shall I do there?

S: Update\_PMat\_At\_Given\_Edge() is the right place to call beagleSetTransitionMatrix(). You don't need to worry about the cases where PhyML calls PMat() directly (these parts of PhyML are deprecated).

  * Does Pij\_rr hold the probabilities, first, and second derivatives? IOW, it is of dimension statecount\*state\*count\*3 ? correct?

S: Pij\_rr holds the transition probabilities for each rate category. Its dimension is therefore mod->ns **mod->ns** mod->ras->n\_catg

  * Why does struct EquFreq have a **next and**prev pointers?

S: the **next and**prev pointers are here to connect to equilibrium frequency vectors that apply to other elements of a data partition (multi-gene analysis).

  * In PhyML, are the substitution rates in ras->gamma\_rr->v ? If so, it seems the gamma\_rr->len is misleading as it is unused; shall I remove it?

S: I'll first take a look at it and will get back to you.

  * Also on a slightly related note, I ran PhyML with parameters "-m GTR -o tlr"... and noticed (via Print\_Model()) that, instead of the default 4 rates, 2 rates were printed? is PhyML overriding the user's preference and modifying mod->ras->n\_catg ?

S: In the first stages of the tree topology estimation, we use 2 classes when the user-required number of classes is > 2 in order to speed up the calculations. We switch back to the required number afterwards.

  * PhyML uses "phydbl" throughout which can either be "float" or "double". However, BEAGLE uses "double" for function arguments -- I suspect I will need to do "float to double" conversion? Any caveats I should be aware of here?

S: I used simple precision to run some tests, which where rather inconclusive if I remember well. You could just disable compiling PhyML with BEAGLE when phydbl is a float.

  * PhyML; Is io->do\_alias\_subpatt ever TRUE in main()? IOW, when is the code at line 187 ever executed?

S: it is executed if you provide the option --alias\_subpatt om the command line.

  * PhyML; can PhyML accept Tips with partials?

S: the partials are here integers rather than floating values for the internal nodes.

  * For PhyML; should I instantiate Beagle with SINGLE or DOUBLE precision? I believe PhyML uses double everywhere?

S: it does by default (i.e. when phydbl is double)

  * It seems the Eigen is only computed **and** updated for the HKY85, GTR, and Custom models. If so, mathematically, how does one compute the Pmatrix for the other models?

S: there are analytical formula available for some nucleotide substitution models.

  * Unconstrained log-likelihood?

S: that's the likelihood calculated under a multinomial model which does not take into account the phylogeny.

  * Recall, the user specifies the number of rate categories and now suppose we estimate an alpha; so now we have a Gamma distribution. But now, how do we actually determine the 4 rates? do we simply randomly sample 4 times from the distribution?

S: the Gamma distibution is discretized, i.e., cut into pieces of equal probabilities. Relative rates are then chosen as the 'middle' (i.e., the median or the mean) of each such piece. Note however that the probabilities for the different classes may not always be equal (I am currently implementing a model where both rates and frequencies are directly estimated from the data).
